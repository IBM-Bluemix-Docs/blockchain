---

copyright:
  years: 2019
lastupdated: "2019-04-23"

subcollection: blockchain

---

{:new_window: target="_blank"}
{:shortdesc: .shortdesc}
{:screen: .screen}
{:codeblock: .codeblock}
{:note: .note}
{:important: .important}
{:tip: .tip}
{:pre: .pre}

# 컴포넌트 관리
{: #ibp-console-govern}

CA, 피어, 순서 지정자, 조직 및 채널을 작성한 후 콘솔을 사용하여 이 컴포넌트를 업데이트할 수 있습니다.
{:shortdesc}

**대상 독자:** 이 주제는 블록체인 네트워크를 작성, 모니터링 및 관리할 책임이 있는 네트워크 운영자를 위해 설계되었습니다.  

## {{site.data.keyword.cloud_notm}} Kubernetes Service가 콘솔과 상호작용하는 방법
{: #ibp-console-govern-iks-console-interaction}

네트워크 운영자는 CPU, 메모리 및 스토리지 사용량을 모니터해야 하고 노드의 작성 또는 크기 변경을 시도하기 **전에** 충분한 리소스가 사용 가능한지 확인해야 합니다.
{:important}

{{site.data.keyword.blockchainfull_notm}} Platform 콘솔의 인스턴스 및 {{site.data.keyword.cloud_notm}} Kubernetes Service 클러스터가 클러스터에서 사용 가능한 리소스에 대해 직접 통신하지 않으므로, 콘솔을 사용하여 컴포넌트를 배치하거나 컴포넌트의 크기를 조정하는 프로세스는 다음 패턴을 따라야 합니다. 

1. **배치할 크기를 지정하십시오**. 콘솔에서 CA, 피어 및 순서 지정자를 위한 **리소스 할당** 패널은 각 노드마다 기본 CPU, 메모리 및 스토리지 할당을 제공합니다. 유스 케이스에 따라 이 값을 조정해야 할 수도 있습니다. 확실하지 않은 경우 기본 할당부터 시작하고 필요한 경우 기본 할당을 조정하십시오. 이와 마찬가지로 **리소스 할당** 패널은 기존 리소스 할당을 표시합니다. 

  클러스터에 필요한 스토리지 및 컴퓨팅의 양은 이 목록 다음에 있는 차트를 참조하십시오. 여기에는 피어, 순서 지정자 및 CA에 대한 현재 기본값이 포함되어 있습니다. 

2. **{{site.data.keyword.cloud_notm}} Kubernetes Service 클러스터에 충분한 리소스가 있는지 확인하십시오**. Kubernetes 리소스를 모니터하려면 {{site.data.keyword.cloud_notm}} Kubernetes 대시보드와 결합하여 [{{site.data.keyword.cloud_notm}} SysDig ![외부 링크 아이콘](../images/external_link.svg "외부 링크 아이콘")](https://www.ibm.com/cloud/sysdig "IBM Cloud Monitoring with Sysdig") 도구를 사용하는 것이 좋습니다. 리소스를 배치하거나 리소스의 크기를 조정하는 데 충분한 공간이 부족한 경우 {{site.data.keyword.cloud_notm}} Kubernetes Service 클러스터의 크기를 늘려야 합니다. 클러스터의 크기를 늘리는 방법에 대한 자세한 정보는 [클러스터 스케일링 ![외부 링크 아이콘](../images/external_link.svg "외부 링크 아이콘")](/docs/containers?topic=containers-ca#ca "클러스터 스케일링")을 참조하십시오. 클러스터에 충분한 공간이 있으면 3단계로 계속 진행할 수 있습니다. 
3. **콘솔을 사용하여 노드를 배치하거나 노드의 크기를 조정하십시오**. 팟(Pod)이 실행 중인 작업자 노드에 리소스가 부족한 경우 새 대형 작업자 노드를 클러스터에 추가한 후 기존 작업 노드를 삭제할 수 있습니다. 

| **컴포넌트**(모든 컨테이너) | CPU(millicpus) | CPU(CPU) | 메모리(MB) | 메모리(GB) | 스토리지(GB) |
|--------------------------------|--------------------|---------------|-----------------------|-----------------------|------------------------|
| **피어**                       | 1100               | 1.1           | 2200                  | 2.2                   | 200                    |
| **CA**                         | 300                | .3            | 600                   | .6                    | 10                     |
|**순서 지정자**                    | 450                | .45           | 900                   | .9                    | 100                    |

사용자가 노드를 완전히 중단하거나 삭제하지 않고 비용을 최소화하려는 경우 노드를 최소 0.001CPU(1milliCPU)까지 아래로 스케일링항 수 있습니다. 이 CPU 양을 사용하는 경우에는 이 노드가 작동하지 않습니다.
{:important}

## 리소스 할당
{: #ibp-console-govern-allocate-resources}

무료 클러스터의 사용자는 노드와 연관된 컨테이너에 대해 **기본 크기를 사용해야** 하지만, 유료 클러스터의 사용자는 노드 작성 중에 이 값을 설정할 수 있습니다. 

콘솔의 **리소스 할당** 패널은 노드 작성과 연관된 여러 필드에 대한 기본값을 제공합니다. 이 값에는 시작하는 데 유용한 방법이 표시되어 있으므로 해당 값이 선택됩니다. 그러나 모든 유스 케이스는 다릅니다. 이 주제에서는 이 값을 찾기 위한 방법을 설명하지만, 궁극적으로 사용자는 노드를 모니터하고 노드에 적합한 크기를 찾아야 합니다. 그러므로 사용자가 기본값과는 다른 값이 필요하다고 확신하는 경우를 제외하고는 실현 가능한 측면에서 이 기본값을 사용하고 나중에 이 값을 조정하는 것이 좋습니다. {{site.data.keyword.blockchainfull_notm}} Platform을 기반으로 한 Hyperledger Fabric의 성능 및 스케일에 대한 개요는 [Answering your questions on Hyperledger Fabric performance and scale ![외부 링크 아이콘](../images/external_link.svg "외부 링크 아이콘")](https://www.ibm.com/blogs/blockchain/2019/01/answering-your-questions-on-hyperledger-fabric-performance-and-scale/ "Hyperledger Fabric 성능 및 스케일 관련 블로그")을 참조하십시오.

노드와 연관된 모든 컨테이너에 **CPU** 및 **메모리**가 있으며, 피어, 순서 지정자 및 CA와 연관된 특정 컨테이너에도 **스토리지**가 있습니다. {{site.data.keyword.cloud_notm}}에서 사용할 수 있는 여러 스토리지 옵션에 대한 자세한 정보는 [스토리지 옵션 ![외부 링크 아이콘](../images/external_link.svg "외부 링크 아이콘")](/docs/containers?topic=containers-kube_concepts#kube_concepts "Kubernetes 스토리지 옵션")을 참조하십시오. 사용할 스토리지 클래스를 결정하기 전에 옵션을 충분히 검토하십시오. 

콘솔 및 {{site.data.keyword.cloud_notm}} Kubernetes Service 대시보드를 사용하여 CPU와 메모리를 변경할 수 있으나, 노드를 작성한 후에는 {{site.data.keyword.cloud_notm}} CLI를 사용해야만 스토리지를 나중에 변경할 수 있습니다.
{:note}

모든 노드에는 콘솔과 노드 간 통신 계층을 부트스트랩하는 gRPC 웹 프록시 컨테이너가 있습니다. 이 컨테이너는 고정된 리소스 값을 보유하고 있으며 리소스 할당 패널에 포함되어 있음에 따라 노드를 배치하기 위해 Kubernetes 클러스터에 필요한 공간의 양을 정확하게 예측하여 해당 값을 제공합니다. 이 컨테이너의 값을 변경할 수 없으므로 다음 절에서는 gRPC 웹 프록시를 다루지 않습니다. 

### 인증 기관(CA)
{: #ibp-console-govern-CA}

트랜잭션 프로세스와 크게 연관된 피어 및 순서 지정자와는 달리 CA는 ID 등록 및 MSP 작성에만 연관됩니다. 이는 더 적은 양의 CPU와 메모리가 필요함을 의미합니다. CA에 중점을 두기 위해 사용자는 요청을 모두 처리해야 하거나(API 및 스크립트를 사용할 가능성이 있음) 너무 많은 인증서를 발행하여 스토리지가 부족한 상태입니다. 일반적인 오퍼레이션에서는 두 사항 모두 발생하면 안 되며, 항상 이들 값은 특정 유스 케이스의 요구사항을 반영해야 합니다. 

CA에는 조정할 수 있는 하나의 연관된 컨테이너만 있습니다. 

* **CA 자체**: CA가 발행하는 모든 인증서의 사본을 저장할 뿐만 아니라 내부 CA 프로세스(예: 노드 및 사용자 등록)를 캡슐화합니다. 

#### 작성 중 CA 크기 지정
{: #ibp-console-govern-CA-sizing-creation}

gRPC 웹 프록시 서버의 값을 변경할 수 없으므로 CA에는 유의해야 하는 값이 포함된 하나의 컨테이너만 있습니다. 

| 리소스 | 증가 조건 |
|-----------------|-----------------------|
| **CA 컨테이너 CPU 및 메모리** | CA에 등록이 쇄도할 것으로 예상되는 경우 |
| **CA 스토리지** | 이 CA를 사용하여 많은 수의 사용자 및 애플리케이션을 등록할 계획인 경우 |

### 피어
{: #ibp-console-govern-peers}

피어에는 조정할 수 있는 세 가지 연관된 컨테이너가 있습니다. 

- **피어 자체**: 속하는 모든 채널에 대해 내부 피어 프로세스(예: 트랜잭션 유효성 검증) 및 블록체인(즉, 트랜잭션 히스토리)을 캡슐화합니다. 피어의 스토리지에는 피어에 설치된 스마트 계약도 포함됩니다. 
- **CouchDB**: 여기서 피어의 상태 데이터베이스가 저장됩니다. 각 채널에 별도의 상태 데이터베이스가 있음을 상기하십시오. 
- **스마트 계약**: 트랜잭션 중에 관련 스마트 계약이 "호출됨"(즉, 실행)을 상기하십시오. 피어에 설치하는 모든 스마트 계약은 Docker-in-Docker 컨테이너로 알려진 스마트 계약 컨테이너 내부에 있는 별도의 컨테이너에서 실행됩니다. 

#### 작성 중 피어 크기 지정
{: #ibp-console-govern-peers-sizing-creation}

[Kubernetes가 콘솔과 상호작용 하는 방법](#ibp-console-govern-iks-console-interaction)의 절에서 설명한 대로, 이 피어 컨테이너에 기본값을 사용하고 나중에 피어 컨테이너의 활용 방식이 명확해질 때 조정하는 것이 좋습니다. 

| 리소스 | 증가 조건 |
|-----------------|-----------------------|
| **피어 컨테이너 CPU 및 메모리** |높은 트랜잭션 처리량을 즉시 예측하는 경우 |
| **피어 스토리지** | 이 피어에 다수의 스마트 계약을 설치하고 여러 채널에 참여할 것을 예측하는 경우. 이 스토리지는 피어가 참여할 모든 채널에서 스마트 계약을 저장하는 데에도 사용됨을 상기하십시오. 10,000바이트(10k) 범위의 "소형" 트랜잭션을 예측한다는 점에 유의하십시오. 기본 스토리지가 100G이면 이는 총 트랜잭션을 최대 1000만으로 늘려야 피어 스토리지에 적합함을 의미합니다(실제로, 트랜잭션이 크기에 따라 달라질 수 있고 해당 수에 스마트 계약이 포함되지 않음에 따라 최대 수는 이 값보다 미만임). 따라서 100G은 필요한 스토리지보다 훨씬 더 많아 보이지만, 스토리지는 상대적으로 비싸지 않으며 스토리지를 늘리는 프로세스가 CPU 또는 메모리를 늘리는 프로세스보다 훨씬 더 어렵다는 점을 기억하십시오. |
| **CouchDB 컨테이너 CPU 및 메모리** | 대형 상태 데이터베이스에 맞게 조회의 볼륨이 높아질 것을 예측하는 경우. [인덱스 ![외부 링크 아이콘](../images/external_link.svg "외부 링크 아이콘")](https://hyperledger-fabric.readthedocs.io/en/release-1.4/couchdb_as_state_database.html#couchdb-indexes "Hyperledger Fabric 인덱스 문서")를 사용하면 이 현상이 다소 완화될 수 있습니다. 그렇지만 볼륨이 높으면 CouchDB가 한계에 도달할 수 있으며, 이에 따라 조회 및 트랜잭션 제한시간이 초과될 수 있습니다. |
| **CouchDB(원장 데이터) 스토리지** | 많은 채널에서 처리량이 높아질 것을 예측하며 인덱스를 사용할 계획이 없는 경우. 그러나 피어 스토리지와 마찬가지로 기본 CouchDB 스토리지는 100G이며, 이는 상당한 양입니다. |
| **스마트 계약 컨테이너 CPU 및 메모리** | 채널에서 처리량이 높아질 것을 예측하는 경우(특히 다중 스마트 계약이 즉시 즉시 호출될 경우) |

### 노드 순서 지정
{: #ibp-console-govern-ordering-nodes}

순서 지정자가 상태 DB를 유지보수하지 않고 스마트 계약을 호스팅하지 않으므로 피어보다 더 적은 컨테이너가 필요합니다. 그러나 블록체인이 채널 구성을 저장하는 위치임에 따라 순서 지정자는 블록체인(트랜잭션 히스토리)을 호스팅하지 않고, 순서 지정자는 해당 역할을 수행하기 위해 최신 채널 구성을 알고 있어야 합니다. 

CA와 마찬가지로 순서 지정자에는 조정할 수 있는 하나의 연관된 컨테이너만 있습니다. 

* **순서 지정자 자체**: 호스팅하는 모든 채널에 대해 내부 순서 지정자 프로세스(예: 트랜잭션 유효성 검증) 및 블록체인을 캡슐화합니다. 

#### 작성 중 순서 지정자 크기 지정
{: #ibp-console-govern-peers-sizing-creation}

[Kubernetes가 콘솔과 상호작용 하는 방법](#ibp-console-govern-iks-console-interaction)의 절에서 설명한 대로, 이 순서 지정자 컨테이너에 기본값을 사용하고 나중에 순서 지정자 컨테이너의 활용 방식이 명확해질 때 조정하는 것이 좋습니다. 

| 리소스 | 증가 조건 |
|-----------------|-----------------------|
| **순서 지정자 컨테이너 CPU 및 메모리** |높은 트랜잭션 처리량을 즉시 예측하는 경우 |
| **순서 지정자 스토리지** | 이 순서 지정자가 많은 채널에서 순서 지정 서비스의 일부가 될 것을 예측하는 경우. 순서 지정자가 일부가 되는 모든 채널에 대한 블록체인의 사본을 보관하고 있음을 상기하십시오. 순서 지정자의 기본 스토리지는 피어 자체의 컨테이너와 동일한 100G입니다. |

순서 지정자 노드의 CPU 및 메모리를 피어 크기의 두 배로 만드는 것은 필수는 아니지만 우수 사례로 간주됩니다. SOLO 순서 지정 노드가 과도하게 설정되는 경우 제한시간이 초과되고 트랜잭션 감소가 시작되며, 이에 따라 트랜잭션을 다시 제출해야 합니다. 이는 지속적인 운영을 위해 노력하는 단일 피어보다 네트워크에 더욱 나쁜 영향을 줄 수 있습니다. Raft 순서 지정 서비스 구성에서 과도하게 설정된 선행 노드로 하트비트 메시지 전송, 선행 선출 트리거 및 트랜잭션 순서 지정의 임시 중단이 중지될 수 있습니다. 이와 유사하게, 팔로워 노드는 메시지를 누락시킬 수 있고 아무 것도 필요하지 않은 선행 선출을 트리거하려고 할 수 있습니다.
{:important}

## 리소스 재할당
{: #ibp-console-govern-reallocate-resources}

노드의 크기를 조정한 후에는 컨테이너가 다시 빌드됨에 따라 크기 변경이 적용되기 전에 지연이 발생할 수 있습니다.
{:important}

위에서 언급한 대로, Kubernetes 리소스 사용량을 모니터하려면 {{site.data.keyword.cloud_notm}} Kubernetes 대시보드를 결합하여 [{{site.data.keyword.cloud_notm}} SysDig ![외부 링크 아이콘](../images/external_link.svg "외부 링크 아이콘")](https://www.ibm.com/cloud/sysdig "{{site.data.keyword.cloud_notm}} Monitoring with Sysdig") 도구를 사용하는 것이 좋습니다. 작업자 노드가 리소스 외부에서 실행되는 경우 새 대형 작업자 노드를 클러스터에 추가한 후 기존 작업 노드를 삭제할 수 있습니다. {:note}

{{site.data.keyword.cloud_notm}} Kubernetes Service에 충분한 리소스를 배치하는 것이 더 쉽고 먼저 {{site.data.keyword.cloud_notm}} Kubernetes Service에 대한 배치를 늘리지 않고도 원하는 대로 팟(Pod) 및 작업자 노드를 확장할 수 있지만, {{site.data.keyword.cloud_notm}} Kubernetes Service의 배치가 늘어나면 비용도 증가합니다. 사용자는 이 옵션을 신중하게 고려하고 사용자가 선택하는 옵션에 관계 없이 절충안을 알고 있어야 합니다. 

노드와 연관되는 컨테이너에 지정하는 리소스를 재할당하기 위해 다음 방법 중 하나를 사용할 수 있습니다.

1. **{{site.data.keyword.cloud_notm}} Kubernetes Service Autoscaler**를 사용하십시오. Autoscaler는 팟(Pod) 스펙 설정 및 리소스 요청에 대한 응답으로 작업자 노드를 위 또는 아래로 스케일링합니다. {{site.data.keyword.cloud_notm}} Kubernetes Service Autoscaler 및 설정 방법에 대한 자세한 정보는 IBM 문서의 {{site.data.keyword.cloud_notm}}[클러스터 스케일링 ![외부 링크 아이콘](../images/external_link.svg "외부 링크 아이콘")](/docs/containers?topic=containers-ca#ca "클러스터 스케일링")을 참조하십시오. 리소스를 조정하기 위해 Autoscaler를 사용하면 {{site.data.keyword.cloud_notm}} Kubernetes Service 계정에 대한 비용이 부과되며, 이는 사용량에 따라 달라집니다. 
2. **수동으로 스케일링하십시오**. 여기에는 콘솔 및 {{site.data.keyword.cloud_notm}} Kubernetes Service 클러스터의 사용량 모니터링이 포함됩니다. Autoscaler 사용보다 더 많은 수동 단계가 포함되지만, 사용자가 {{site.data.keyword.cloud_notm}} Kubernetes Service 계정에 부과되는 요금에 대해서는 언제나 확실히 알고 있다는 장점이 있습니다. 

수동으로 스케일링하려면 **노드** 페이지에서 조정할 노드를 클릭한 후 **사용량** 탭을 클릭하십시오. **재할당**이라는 단추가 표시되며, 이를 통해 노드 작성 시 표시된 탭과 매우 유사한 **리소스 재할당** 탭이 실행됩니다. 사용 가능한 리소스의 양을 줄이려면 단순히 더 적은 값만 제공하고 탭의 **리소스 재할당**을 클릭한 후 **요약** 페이지를 클릭하십시오. 

노드에 맞게 CPU와 메모리의 양을 늘리려면 **리소스 할당** 탭을 사용하여 값을 늘리십시오. 페이지 맨 아래에 있는 흰색 상자에서 새 값이 추가됩니다. **리소스 재할당**을 클릭하면 **요약** 페이지에서 이 값이 **VPC** 양으로 변환되며 이는 청구서를 계산하는 데 사용됩니다. 그런 다음 {{site.data.keyword.cloud_notm}} Kubernetes Service 대시보드로 이동하여 클러스터에 이 재할당을 처리하는 데 충분한 리소스가 있는지 확인해야 합니다. 리소스가 충분하면 **리소스 재할당**을 클릭할 수 있습니다. 리소스가 충분하지 않으면 {{site.data.keyword.cloud_notm}} Kubernetes Service 대시보드를 사용하여 클러스터의 크기를 늘려야 합니다. 

스토리지를 늘리는 데 사용할 메소드는 클러스터에 대해 선택한 스토리지 클래스에 따라 달라집니다. 스토리지를 늘리는 데 대한 정보는 [스토리지 옵션 ![외부 링크 아이콘](../images/external_link.svg "외부 링크 아이콘")](/docs/containers?topic=containers-kube_concepts#kube_concepts "스토리지 옵션) 문서를 참조하십시오.

콘솔을 사용하여 늘릴 수 있는 CPU 및 메모리와는 달리({{site.data.keyword.cloud_notm}} Kubernetes Service 클러스터에 사용 가능한 리소스가 있는 경우) {{site.data.keyword.cloud_notm}} CLI를 사용하여 노드의 스토리지를 늘려야 합니다. 이를 수행하는 데 필요한 튜토리얼은 [기존 스토리지 디바이스의 크기 및 IOPS 변경![외부 링크 아이콘](../images/external_link.svg "외부 링크 아이콘")](/docs/containers?topic=containers-file_storage#file_change_storage_configuration "기존 스토리지 디바이스의 크기 및 IOPS 변경")을 참조하십시오.

## 순서 지정자 조정
{: #ibp-console-govern-orderer-tuning}

블록체인 플랫폼의 성능은 트랜잭션 크기, 블록 크기, 네트워크 크기 및 하드웨어 한계와 같이 여러 변수의 영향을 받을 수 있습니다. 순서 지정자 노드에는 순서 지정자 처리량 및 성능을 제어하는 데 함께 사용될 수 있는 조정 매개변수 세트가 포함됩니다. 이 매개변수를 사용하면 순서 지정자가 트랜잭션을 처리하는 방식을 사용자 정의할 수 있으며, 이는 빈도 수는 높지만 소형 트랜잭션이 있는지 아니면 빈도 수는 적지만 대형 트랜잭션(더 적은 횟수로 도달함)에 따라 달라집니다. 기본적으로 트랜잭션 크기, 양 및 도달률을 기반으로 블록을 줄이는 시기를 결정할 수 있는 제어 권한이 제공됩니다. 

다음 매개변수는 **노드** 탭에서 순서 지정자 노드를 클릭한 후 **설정** 아이콘을 클릭하여 콘솔에서 사용할 수 있습니다. **고급** 단추를 클릭하여 순서 지정자를 위한 **고급 채널 구성**을 여십시오. 

### 일괄처리 크기
{: #ibp-console-govern-orderer-tuning-batch-size}

블록을 줄이는 시기를 제어하려면 블록의 최대 트랜잭션 수 설정과 블록 크기 자체의 결합을 기반으로 하여 다음 세 가지 매개변수가 함께 적용됩니다. 

- **절대 최대 바이트**  
  이 값을 순서 지정자가 줄일 수 있는 최대 블록 크기(바이트)로 설정합니다. 트랜잭션은 `Absolute max bytes` 값보다 클 수 없습니다. 일반적으로 이 설정은 안전하게 `Preferred max bytes`보다 2 - 10배 더 클 수 있습니다.     
  **참고**: 허용되는 최대 크기는 99MB입니다.
- **최대 메시지 수**   
  이 값을 단일 불록에 포함할 수 있는 최대 트랜잭션 수로 설정합니다. 
- **원하는 최대 바이트**  
  이 값을 `Absolute max bytes` 미만의 이상적인 블록 크기(바이트)로 설정합니다. 보증이 포함되지 않은 최소 트랜잭션 크기는 약 1KB입니다. 필요한 보증마다 1KB를 추가하면 일반적인 트랜잭션 크기는 대략 3 - 4KB가 됩니다. 그러므로 `Preferred max bytes`의 값을 약 `Max message count * expected averaged tx size`로 설정하는 것이 좋습니다. 런타임 시 가능한 한 블록은 이 크기를 초과하지 않습니다. 트랜잭션이 블록으로 인해 이 크기가 초과하는 지점에 도달한 경우 블록은 줄어들고 해당 트랜잭션에 맞게 새 블록이 작성됩니다. 트랜잭션이 `Absolute max bytes`를 초과하지 않고 이 값을 초과하는 지점에 도달하는 경우 트랜잭션이 포함됩니다. 블록이 `Preferred max bytes`보다 큰 지점에 도달하는 경우 단일 트랜잭션만 포함되고 해당 트랜잭션 크기가 `Absolute max bytes`보다 클 수 없습니다. 

이 매개변수는 순서 지정자의 처리량을 최적화하도록 구성될 수 있습니다. 

### 일괄처리 제한시간 초과
{: #ibp-console-govern-orderer-tuning-batch-timeout}

**제한시간 초과** 값을 첫 번째 트랜잭션이 블록을 줄이기 전에 도달한 후 기다리는 시간(초)으로 설정하십시오. 이 값을 너무 짧게 설정하면 일괄처리를 원하는 크기로 채울 수 없게 됩니다. 이 값을 너무 길게 설정하면 순서 지정자가 블록 및 전체 성능이 저하될 때까지 기다릴 수 있습니다. 일반적으로 최소 `max message count / maximum transactions per second`로 `Batch timeout` 값을 설정하는 것이 좋습니다.

이 매개변수를 수정하는 경우 순서 지정자의 기존 채널 작동에는 영향을 주지 않습니다. 대신, 순서 지정자 구성의 변경사항은 이 순서 지정자에서 작성하는 새 채널에만 적용됩니다.
{:important}

## 채널 수정
{: #ibp-console-govern-channels}

### 앵커 피어 구성
{: #ibp-console-govern-channels-anchor-peers}

서비스 발견 및 개인용 데이터가 작동하려면 교차 조직의 [gossip ![외부 링크 아이콘](../images/external_link.svg "외부 링크 아이콘")](https://hyperledger-fabric.readthedocs.io/en/release-1.4/gossip.html "gossip 데이터 분배 프로토콜")을 사용할 수 있어야 하므로 각 조직에 대해 앵커 피어가 존재해야 합니다. 이 앵커 피어는 특별한 **유형**의 피어가 아니지만 조직이 다른 조직에 알려져 있고 부트스트랩 교차 조직의 gossip을 수행하는 피어입니다. 그러므로 하나 이상의 [앵커 피어 ![외부 링크 아이콘](../images/external_link.svg "외부 링크 아이콘")](https://hyperledger-fabric.readthedocs.io/en/release-1.4/gossip.html#anchor-peers "앵커 피어")가 콜렉션의 각 조직마다 정의되어야 합니다. 

피어를 앵커 피어가 되도록 구성하려면 **채널** 탭을 클릭하고 스마트 계약이 인스턴스화되는 채널을 여십시오. 
 - **채널 세부사항** 탭을 클릭하십시오. 
 - 아래로 스크롤하여 앵커 피어 테이블로 이동한 후 **앵커 피어 추가**를 클릭하십시오.
 - 조직에서 앵커 피어의 역할을 수행할 콜렉션 정의의 각 조직에서 하나 이상의 피어를 선택하십시오. 중복성의 이유로 콜렉션의 각 조직에서 둘 이상의 피어를 선택할 것을 고려할 수 있습니다. 
