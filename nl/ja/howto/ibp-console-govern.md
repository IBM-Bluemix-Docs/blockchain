---

copyright:
  years: 2019
lastupdated: "2019-04-23"

subcollection: blockchain

---

{:new_window: target="_blank"}
{:shortdesc: .shortdesc}
{:screen: .screen}
{:codeblock: .codeblock}
{:note: .note}
{:important: .important}
{:tip: .tip}
{:pre: .pre}

# コンポーネントの管理
{: #ibp-console-govern}

CA、ピア、順序付けプログラム、組織、チャネルの作成後は、コンソールを使用してそれらのコンポーネントを更新できます。
{:shortdesc}

**対象者:** このトピックは、ブロックチェーン・ネットワークの作成、モニター、管理を担当するネットワーク・オペレーターを対象に設計されています。  

## {{site.data.keyword.cloud_notm}} Kubernetes Service とコンソールの対話
{: #ibp-console-govern-iks-console-interaction}

ノードを作成したりノードのサイズを変更したりする**前に**、ネットワーク・オペレーターが、CPU、メモリー、ストレージの使用量をモニターし、十分なリソースがあることを確認する必要があります。
{:important}

{{site.data.keyword.blockchainfull_notm}} Platform コンソールのインスタンスと {{site.data.keyword.cloud_notm}} Kubernetes Service クラスターは、クラスター内のリソースについての情報を直接やり取りするわけではないので、このコンソールでコンポーネントのデプロイおよびサイズ変更を行うには、次のパターンのプロセスを踏む必要があります。

1. **作成するデプロイメントのサイズを指定します**。コンソールの CA、ピア、順序付けプログラムについての**「リソース割り振り」**パネルには、各ノードのデフォルトの CPU、メモリー、ストレージの割り当て量が表示されます。ユース・ケースに応じてこれらの値を調整します。よくわからない場合は、デフォルトの割り振りを使用して始め、必要量がわかってから調整します。同様に、**「リソース再割り振り (Resource reallocation)」**パネルには、リソースの既存の割り当て量が表示されます。

  クラスターに必要なストレージ量およびコンピュート量の目安として、このリストの後の表を参照してください。ピア、順序付けプログラム、CA の現在のデフォルト値を記載しています。

2. **{{site.data.keyword.cloud_notm}} Kubernetes Service クラスターに十分なリソースがあることを確認します**。Kubernetes リソースをモニターするには、{{site.data.keyword.cloud_notm}} Kubernetes ダッシュボードと [{{site.data.keyword.cloud_notm}} SysDig ![外部リンクのアイコン](../images/external_link.svg "外部リンクのアイコン")](https://www.ibm.com/cloud/sysdig "IBM Cloud Monitoring with Sysdig") ツールを組み合わせて使用することをお勧めします。リソースのデプロイまたはサイズ変更を行えるだけの十分なスペースがクラスターにない場合は、{{site.data.keyword.cloud_notm}} Kubernetes Service クラスターのサイズを増やす必要があります。クラスターのサイズを増やす方法について詳しくは、[クラスターのスケーリング ![外部リンクのアイコン](../images/external_link.svg "外部リンクのアイコン")](/docs/containers?topic=containers-ca#ca "クラスターのスケーリング") を参照してください。クラスターに十分なスペースがあれば、手順 3 に進むことができます。
3. **コンソールを使用してノードのデプロイまたはサイズ変更を行います**。ポッドを実行しているワーカー・ノードのリソースが不足している場合は、より大きな新規ワーカー・ノードをクラスターに追加してから、既存のワーカー・ノードを削除します。

| **コンポーネント** (すべてのコンテナー) | CPU (millicpu) | CPU (CPU) | メモリー (メガバイト) | メモリー (ギガバイト) | ストレージ (ギガバイト) |
|--------------------------------|--------------------|---------------|-----------------------|-----------------------|------------------------|
| **ピア**                       | 1100               | 1.1           | 2200                  | 2.2                   | 200                    |
| **CA**                         | 300                | .3            | 600                   | .6                    | 10                     |
| **順序付けプログラム**                    | 450                | .45           | 900                   | .9                    | 100                    |

ノードを完全に廃止したり除去したりすることなく料金を最低限に抑えられるように、ノードは最小の 0.001 CPU (1 milliCPU) までスケールダウンできるようになっています。この量の CPU では、ノードは正常に機能しなくなることに注意してください。
{:important}

## リソースの割り振り
{: #ibp-console-govern-allocate-resources}

無料クラスターのユーザーはノードに関連付けたコンテナーに**デフォルト・サイズを使用する必要があります**が、有料クラスターのユーザーはノードの作成時にサイズ値を設定できます。

コンソールの**「リソース割り振り」**パネルでは、ノードの作成に関係する複数のフィールドにデフォルト値が設定されています。これらの値が選択されているのは、初期値としてお勧めする値であるからです。しかし、さまざまなユース・ケースがあります。このトピックではこれらの値について検討する方法を説明しますが、最終的には、ユーザーがノードをモニターして適切なサイズを見つける必要があります。そのため、デフォルト値以外の値が必要であることが明らかな場合を除き、これらのデフォルト値を使用し、後で値を調整するというのが実用的な手法です。{{site.data.keyword.blockchainfull_notm}} Platform のベースである Hyperledger Fabric のパフォーマンスおよびスケールの概要については、[Answering your questions on Hyperledger Fabric performance and scale ![外部リンクのアイコン](../images/external_link.svg "外部リンクのアイコン")](https://www.ibm.com/blogs/blockchain/2019/01/answering-your-questions-on-hyperledger-fabric-performance-and-scale/ "Hyperledger Fabric のパフォーマンスおよびスケールについてのブログ") を参照してください。

ノードに関連するすべてのコンテナーに **CPU** と**メモリー**が割り当てられますが、ピア、順序付けプログラム、CA に関連する特定のコンテナーには**ストレージ**も割り当てられます。{{site.data.keyword.cloud_notm}} で利用できる各種ストレージ・オプションについて詳しくは、[ストレージのオプション ![外部リンクのアイコン](../images/external_link.svg "外部リンクのアイコン")](/docs/containers?topic=containers-kube_concepts#kube_concepts "Kubernetes ストレージのオプション") を参照してください。使用するストレージ・クラスを決定する前に、オプションについてしっかりと理解してください。

CPU とメモリーの変更はコンソールおよび {{site.data.keyword.cloud_notm}} Kubernetes Service ダッシュボードで行えますが、ノードを作成した後のストレージの変更は、{{site.data.keyword.cloud_notm}} CLI でしか行えません。
{:note}

すべてのノードには、コンソールとノードの間の通信層を開始する gRPC Web プロキシー・コンテナーがあります。このコンテナーのリソース値は固定ですが、ノードのデプロイのために Kubernetes クラスターのスペース所要量を正確に見積もれるように「リソース割り振り」パネルに表示されます。このコンテナーの値は変更できないので、以下のセクションでは gRPC Web プロキシーについては説明しません。

### 認証局 (CA)
{: #ibp-console-govern-CA}

トランザクション処理に積極的に関わるピアおよび順序付けプログラムとは異なり、CA は ID の登録とエンロール、および MSP の作成にのみ関わります。つまり、CA に必要な CPU とメモリーは他より少なくなります。CA に負荷がかかるとしたら、CA に (API およびスクリプトを使用するなどの方法で) 大量の要求を送信したり、大量の証明書を発行してストレージが不足したりする状況です。通常の操作ではこのような状況は起こりません。ただし、常に、具体的なユース・ケースの要件を反映した値にする必要があります。

CA に関連する調整可能なコンテナーは 1 つだけです。

* **CA 自体**: ノードおよびユーザーの登録とエンロール、発行したすべての証明書のコピーの保管など、内部 CA 処理をカプセル化します。

#### 作成時の CA のサイズ設定
{: #ibp-console-govern-CA-sizing-creation}

gRPC Web プロキシー・サーバーの値は変更できないので、値を検討しなければならない CA のコンテナーは 1 つだけです。

| リソース | 増やす状況 |
|-----------------|-----------------------|
| **CA コンテナーの CPU とメモリー** | CA が大量の登録およびエンロールを行うことが予想される場合。 |
| **CA のストレージ** | この CA を使用して大量のユーザーおよびアプリケーションを登録する場合。 |

### ピア
{: #ibp-console-govern-peers}

ピアに関連する調整可能なコンテナーは 3 つあります。

- **ピア自体**: 参加するすべてのチャネルについてのピア内部処理 (トランザクションの検証など) およびブロックチェーン (つまり、トランザクション履歴) をカプセル化します。ピアのストレージには、ピアにインストールされたスマート・コントラクトも含まれます。
- **CouchDB**: ピアの状態データベースが格納される場所。チャネルごとに別の状態データベースが存在することに注意してください。
- **スマート・コントラクト**: トランザクションの中では、関連するスマート・コントラクトの「呼び出し」(つまり、実行) が行われます。ピアにインストールしたすべてのスマート・コントラクトは、スマート・コントラクト・コンテナーの内部にある別のコンテナー (Docker-in-Docker コンテナー) で実行されます。

#### 作成時のピアのサイズ設定
{: #ibp-console-govern-peers-sizing-creation}

[Kubernetes とコンソールの対話](#ibp-console-govern-iks-console-interaction)についてのセクションで触れたように、これらのピア・コンテナーにはデフォルト値を使用し、使用量が明らかになってから調整することをお勧めします。

| リソース | 増やす状況 |
|-----------------|-----------------------|
| **ピア・コンテナーの CPU とメモリー** | すぐに高いトランザクション・スループットを期待する場合。 |
| **ピアのストレージ** | このピアに多くのスマート・コントラクトをインストールし、ピアを多くのチャネルに参加させる場合。このストレージには、ピアが参加するすべてのチャネルのスマート・コントラクトも格納されることに注意してください。小規模トランザクションを 10,000 バイト (10 K 単位) の範囲で見積もっていることを念頭に置いてください。デフォルトのストレージは 100 G であるため、拡張しなくても合計 1,000 万件のトランザクションをピア・ストレージに格納できることになります (実際は、多様なサイズのトランザクションがあるうえに、この数にはスマート・コントラクトが含まれていないので、最大数はこれよりも少なくなります)。そのため 100 G が必要以上に大きいストレージのように思えるかもしれませんが、ストレージが比較的安価であること、また、CPU やメモリーを増やす場合と比べてストレージを増やす作業は難しいこと (コマンド・ラインが必要です) を忘れないでください。 |
| **CouchDB コンテナーの CPU とメモリー** | 大規模な状態データベースに対して大量の照会が行われることが予想される場合。この状況による影響は、[索引 ![外部リンクのアイコン](../images/external_link.svg "外部リンクのアイコン")](https://hyperledger-fabric.readthedocs.io/en/release-1.4/couchdb_as_state_database.html#couchdb-indexes "Hyperledger Fabric 索引の資料") を使用することで、いくらか軽減できる可能性があります。それでもやはり、大量の照会によって CouchDB に負荷がかかり、照会とトランザクションがタイムアウトになる可能性があります。|
| **CouchDB の (台帳データ用) ストレージ** | 多数のチャネルに高スループットを期待し、索引は使用しない場合。ただし、ピアのストレージと同じく、デフォルトの CouchDB ストレージは 100 G であり、十分な大きさです。 |
| **スマート・コントラクト・コンテナーの CPU とメモリー** | チャネルに高スループットを期待する場合。特に、一度に複数のスマート・コントラクトが呼び出される場合。|

### 順序付けプログラム・ノード
{: #ibp-console-govern-ordering-nodes}

順序付けプログラムは状態データベースを保持せず、スマート・コントラクトをホストしないので、ピアに比べて必要なコンテナー数が少なくなります。ただし、ブロックチェーン (トランザクション履歴) をホストします。なぜなら、ブロックチェーンがチャネル構成の保管場所であり、順序付けプログラムは役割を果たすために最新のチャネル構成を認識する必要があるからです。

CA と同じく、順序付けプログラムに関連する調整可能なコンテナーは 1 つだけです。

* **順序付けプログラム自体**: ホストするすべてのチャネルについての順序付けプログラム内部処理 (トランザクションの検証など) とブロックチェーンをカプセル化します。

#### 作成時の順序付けプログラムのサイズ設定
{: #ibp-console-govern-peers-sizing-creation}

[Kubernetes とコンソールの対話](#ibp-console-govern-iks-console-interaction)のセクションで触れたように、これらの順序付けプログラム・コンテナーにはデフォルト値を使用し、使用量が明らかになってから調整することをお勧めします。

| リソース | 増やす状況 |
|-----------------|-----------------------|
| **順序付けプログラム・コンテナーの CPU とメモリー** | すぐに高いトランザクション・スループットを期待する場合。 |
| **順序付けプログラムのストレージ** | この順序付けプログラムが多数のチャネルの順序付けサービスに参加することが予想される場合。順序付けプログラムは、参加するすべてのチャネルのブロックチェーンのコピーを保持することを忘れないでください。順序付けプログラムのデフォルト・ストレージは、ピア自体のコンテナーと同じく、100 G です。 |

必須ではありませんが、順序付けプログラム・ノードの CPU とメモリーは、ピアのサイズの約 2 倍にすることがベスト・プラクティスと見なされています。単独の順序付けプログラム・ノードの負荷が高すぎると、タイムアウトが発生し、トランザクションのドロップが始まり、トランザクションの再送信が必要になる可能性があります。これにより、単一のピアが稼働できなくなった場合よりも大きな被害をネットワークが受ける可能性があります。Raft の順序付けサービス構成では、リーダー・ノードが高負荷になると、ハートビート・メッセージの送信が停止し、リーダー選考がトリガーされ、トランザクションの順序付けが一時停止します。同様に、フォロワー・ノードではメッセージが欠落し、リーダー選考が不要にトリガーされる可能性があります。
{:important}

## リソースの再割り振り
{: #ibp-console-govern-reallocate-resources}

ノードのサイズを変更すると、コンテナーが再作成されるので、変更が有効になるまでに待ち時間が生じる場合があります。
{:important}

前述のように、[{{site.data.keyword.cloud_notm}} SysDig ![外部リンクのアイコン](../images/external_link.svg "外部リンクのアイコン")](https://www.ibm.com/cloud/sysdig "{{site.data.keyword.cloud_notm}} Monitoring with Sysdig") ツールを {{site.data.keyword.cloud_notm}} Kubernetes ダッシュボードと一緒に使用して、Kubernetes リソースの使用状況をモニターすることをお勧めします。ワーカー・ノードのリソースが不足していることがわかった場合は、より大きな新規ワーカー・ノードをクラスターに追加してから、既存のワーカー・ノードを削除します。
{:note}

最初から {{site.data.keyword.cloud_notm}} Kubernetes Service に十分なリソースをデプロイして、必要に応じてポッドやワーカー・ノードを拡張できるようにしておくほうが、{{site.data.keyword.cloud_notm}} Kubernetes Service のデプロイメントを大きくする必要がないので簡単ですが、{{site.data.keyword.cloud_notm}} Kubernetes Service のデプロイメントが大きくなるほど、費用が高くなります。選択肢について慎重に検討し、どのような選択を行うにしても、生じるトレードオフを認識する必要があります。

ノードに関連するコンテナーに割り当てたリソースを再割り振りする方法には、次の方法があります。

1. **{{site.data.keyword.cloud_notm}} Kubernetes Service 自動スケール機能**を使用します。自動スケール機能を使用すると、ポッド仕様の設定とリソース要求に応じてワーカー・ノードをスケールアップ/ダウンできます。{{site.data.keyword.cloud_notm}} Kubernetes Service 自動スケール機能とその設定方法について詳しくは、IBM {{site.data.keyword.cloud_notm}} 資料の[クラスターのスケーリング ![外部リンクのアイコン](../images/external_link.svg "外部リンクのアイコン")](/docs/containers?topic=containers-ca#ca "クラスターのスケーリング") を参照してください。自動スケール機能でリソースを調整可能にすると、使用量に応じて {{site.data.keyword.cloud_notm}} Kubernetes Service アカウントに課金されるので注意してください。
2. **手動でスケーリングします**。そのためにはコンソールおよび {{site.data.keyword.cloud_notm}} Kubernetes Service クラスターで使用状況をモニターする必要があります。自動スケール機能を使用する場合よりも手動で行う手順が多く必要になりますが、{{site.data.keyword.cloud_notm}} Kubernetes Service アカウントへの課金内容を常に把握できるという利点があります。

手動でスケーリングするには、**「ノード」**ページで調整するノードをクリックし、**「使用量」**タブをクリックします。**「再割り振り (Reallocate)」**ボタンが表示されます。このボタンを押すと、ノードの作成時に表示されたタブと非常によく似た**「リソース割り振り」**タブが表示されます。使用できるリソース量を減らす場合は、低い値を設定してそのタブの**「リソースの再割り振り (Reallocate resources)」**をクリックするだけです。そうすれば、**「サマリー」**ページに結果が表示されます。

ノードの CPU とメモリーを増やす場合は、**「リソース割り振り」**タブを使用して値を増やします。ページ下部にある白色のボックスに、増やした新しい値を入力します。**「リソースの再割り振り (Reallocate resources)」**をクリックすると、**「サマリー」**ページでこの値が **VPC** 量に変換されます。この値を使用して請求額が算出されます。次に、{{site.data.keyword.cloud_notm}} Kubernetes Service ダッシュボードに移動して、この再割り振りのために十分なリソースがクラスターにあることを確認する必要があります。十分ある場合は、**「リソースの再割り振り (Reallocate resources)」**をクリックできます。十分なリソースがない場合は、{{site.data.keyword.cloud_notm}} Kubernetes Service ダッシュボードを使用してクラスターのサイズを増やす必要があります。

ストレージを増やすために使用する方法は、クラスターに選択したストレージ・クラスによって異なります。ストレージを増やす方法については、[ストレージのオプション ![外部リンクのアイコン](../images/external_link.svg "外部リンクのアイコン")](/docs/containers?topic=containers-kube_concepts#kube_concepts "ストレージのオプション) 資料を参照してください。

CPU およびメモリーはコンソールを使用して増やせますが (ただし、{{site.data.keyword.cloud_notm}} Kubernetes Service クラスターに使用可能なリソースがなければなりません)、ノードのストレージを増やすには {{site.data.keyword.cloud_notm}} CLI を使用する必要があります。その方法を示すチュートリアルについては、[既存のストレージ・デバイスのサイズと IOPS の変更 ![外部リンクのアイコン](../images/external_link.svg "外部リンクのアイコン")](/docs/containers?topic=containers-file_storage#file_change_storage_configuration "既存のストレージ・デバイスのサイズと IOPS の変更") を参照してください。

## 順序付けプログラムの調整
{: #ibp-console-govern-orderer-tuning}

ブロックチェーン・プラットフォームのパフォーマンスは、トランザクション・サイズ、ブロック・サイズ、ネットワーク・サイズ、ハードウェアの制限など、多くの変動要素の影響を受けます。順序付けプログラムのノードには、順序付けプログラムのスループットとパフォーマンスを制御するために使用できる一連のチューニング・パラメーターが含まれています。これらのパラメーターを使用して、高い頻度で多数の小さいトランザクションを処理するのか、低い頻度で到着する少数の大きいトランザクションを処理するのかに応じて、順序付けプログラムによるトランザクションの処理方法をカスタマイズできます。基本的には、トランザクションのサイズ、数量、および到着レートに応じてブロックをカットするタイミングを制御できます。

コンソールの**「ノード」**タブで順序付けプログラムのノードをクリックしてから**「設定」**アイコンをクリックすると、以下のパラメーターが表示されます。**「詳細」**ボタンをクリックすると、順序付けプログラムの**「詳細なチャネル構成 (Advanced channel configuration)」**が開きます。

### バッチ・サイズ
{: #ibp-console-govern-orderer-tuning-batch-size}

以下の 3 つのパラメーターが組み合わさって、ブロックをカットするタイミングを制御します。1 つのブロックの最大トランザクション数とブロック・サイズ自体の設定の組み合わせが制御の基礎になります。

- **絶対最大バイト数 (Absolute max bytes)**  
 この値には、順序付けプログラムでカットできる最大ブロック・サイズ (バイト単位) を設定します。`絶対最大バイト数`の値より大きいトランザクションは許可されません。通常、この設定値は`推奨最大バイト数`の 2 倍から 10 倍にすることをお勧めします。    
  **注**: 最大サイズは 99 MB です。
- **最大メッセージ数 (Max message count)**   
 この値には、1 つのブロックに含めることができるトランザクションの最大数を設定します。
- **推奨最大バイト数 (Preferred max bytes)**  
 この値には、理想的なブロック・サイズ (バイト単位) を設定します。`絶対最大バイト数`より小さくする必要があります。エンドースメントを含まない最小のトランザクション・サイズは約 1 KB です。必要なエンドースメントごとに 1 KB を加えると、通常のトランザクション・サイズは約 3 KB から 4 KB になります。そのため、`推奨最大バイト数`の値には、`最大メッセージ数 * 予想される平均トランザクション・サイズ`に近い値を設定することをお勧めします。ランタイム時に、このサイズを超えるブロックが (可能な限り) 作成されなくなります。このサイズを超えるブロックになるトランザクションが到着すると、ブロックはカットされ、そのトランザクション用に新規ブロックが作成されます。ただし、この値を超えても`絶対最大バイト数`を超えないトランザクションが到着した場合は、そのトランザクションは含められます。`推奨最大バイト数`を超えるブロックが到着した場合、そのブロックにはトランザクションは 1 つしか含められません。また、そのトランザクション・サイズが`絶対最大バイト数`を超えることはありません。

これらのパラメーターを組み合わせて構成することで、順序付けプログラムのスループットを最適化できます。

### バッチ・タイムアウト
{: #ibp-console-govern-orderer-tuning-batch-timeout}

**タイムアウト**値に、最初のトランザクションが到着してからブロックをカットするまで待つ時間を秒単位で設定します。この値の設定が小さすぎると、バッチが推奨サイズに達しないリスクがあります。この値の設定が大きすぎると、順序付けプログラムがブロックを待機し、全体的なパフォーマンスが低下する可能性があります。一般には、`バッチ・タイムアウト`の値には、`最大メッセージ数 / 1 秒あたりの最大トランザクション数`以上の値を設定することをお勧めします。

これらのパラメーターを変更しても、順序付けプログラムの既存のチャネルの動作には影響しません。順序付けプログラムの構成に加えた変更は、その順序付けプログラムで作成する新規チャネルにのみ適用されます。
{:important}

## チャネルの変更
{: #ibp-console-govern-channels}

### アンカー・ピアの構成
{: #ibp-console-govern-channels-anchor-peers}

サービス・ディスカバリーとプライベート・データを使用するには組織間[ゴシップ ![外部リンクのアイコン](../images/external_link.svg "外部リンクのアイコン")](https://hyperledger-fabric.readthedocs.io/en/release-1.4/gossip.html "Gossip data dissemination protocol") を有効にする必要があるため、組織ごとにアンカー・ピアが存在しなければなりません。このアンカー・ピアは特殊な**タイプ**のピアではありません。組織が他の組織に周知するピアに過ぎず、組織間ゴシップを開始します。そのため、少なくとも 1 つの[アンカー・ピア ![外部リンクのアイコン](../images/external_link.svg "外部リンクのアイコン")](https://hyperledger-fabric.readthedocs.io/en/release-1.4/gossip.html#anchor-peers "アンカー・ピア") を、コレクション定義に含まれている組織ごとに定義する必要があります。

ピアをアンカー・ピアとして構成するには、**「チャネル」**タブをクリックして、スマート・コントラクトがインスタンス化されたチャネルを開きます。
 - **「チャネルの詳細 (Channel details)」**タブをクリックします。
 - アンカー・ピアの表までスクロールダウンし、**「アンカー・ピアの追加 (Add anchor peer)」**をクリックします。
 - 組織のアンカー・ピアにするピアを、コレクション定義に含まれている各組織から 1 つ以上選択します。冗長性を確保するために、コレクションに含まれている各組織から複数のピアを選択することを検討すると良いでしょう。
