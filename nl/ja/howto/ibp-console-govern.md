---

copyright:
  years: 2019
lastupdated: "2019-05-31"

keywords: network components, IBM Cloud Kubernetes Service, allocate resources, batch timeout, channel update, reallocate resources

subcollection: blockchain

---

{:external: target="_blank" .external}
{:shortdesc: .shortdesc}
{:screen: .screen}
{:codeblock: .codeblock}
{:note: .note}
{:important: .important}
{:tip: .tip}
{:pre: .pre}

# コンポーネントの管理
{: #ibp-console-govern}

CA、ピア、順序付けプログラム、組織、チャネルの作成後は、コンソールを使用してそれらのコンポーネントを更新できます。
{:shortdesc}

ベータ・トライアル・バージョンの {{site.data.keyword.blockchainfull_notm}} Platform for {{site.data.keyword.cloud_notm}} を使用している場合は、コンソールの一部のパネルが、一般出荷可能 (GA) サービス・インスタンスに合わせて最新の情報が記載されている現在の文書とは異なる可能性があります。 最新機能をすべて活用するには、[{{site.data.keyword.blockchainfull_notm}} Platform for {{site.data.keyword.cloud_notm}} の概説](/docs/services/blockchain/howto/ibp-v2-deploy-iks.html#ibp-v2-deploy-iks)の指示に従って、この時点で新しい GA サービス・インスタンスをプロビジョンすることをお勧めします。

**対象者:** このトピックは、ブロックチェーン・ネットワークの作成、モニター、管理を担当するネットワーク・オペレーターを対象に設計されています。

## コンソールと Kubernetes クラスターの対話
{: #ibp-console-govern-iks-console-interaction}

ノードを作成したりノードのサイズを変更したりする**前に**、ネットワーク・オペレーターが、CPU、メモリー、ストレージの使用量をモニターし、十分なリソースがあることを確認する必要があります。
{:important}

{{site.data.keyword.blockchainfull_notm}} Platform コンソールのインスタンスと Kubernetes クラスターは、リソースについての情報を直接やり取りするわけではないので、このコンソールでコンポーネントのデプロイやサイズ変更を行うには、次のパターンのプロセスを踏む必要があります。

1. **作成するデプロイメントのサイズを指定します**。 コンソールの CA、ピア、順序付けプログラムについての**「リソース割り振り」**パネルには、各ノードのデフォルトの CPU、メモリー、ストレージの割り振りが表示されます。 ユース・ケースに応じてこれらの値の調整が必要になることがあります。。 不確かな場合は、デフォルトの割り振りで開始し、ニーズを把握してから調整します。 同様に、**「リソース再割り振り (Resource reallocation)」**パネルには、リソースの既存の割り振りが表示されます。

  クラスターに必要なストレージ量およびコンピュート量の目安として、このリストの後の表を参照してください。ピア、順序付けプログラム、CA の現在のデフォルト値を記載しています。

2. **Kubernetes クラスターに十分なリソースがあることを確認します**。 {{site.data.keyword.cloud_notm}} でホストしている Kubernetes クラスターを使用する場合は、{{site.data.keyword.cloud_notm}} Kubernetes ダッシュボードと [{{site.data.keyword.cloud_notm}} SysDig](https://www.ibm.com/cloud/sysdig){: external} ツールを組み合わせて使用することをお勧めします。リソースのデプロイまたはサイズ変更を行えるだけの十分なスペースがクラスターにない場合は、{{site.data.keyword.cloud_notm}} Kubernetes Service クラスターのサイズを増やす必要があります。 クラスターのサイズを増やす方法について詳しくは、[クラスターのスケーリング](/docs/containers?topic=containers-ca#ca){: external}を参照してください。 {{site.data.keyword.cloud_notm}} 以外のクラウド・プロバイダーを使用している場合は、そのプロバイダーの資料を参照して、クラスターをスケーリングする必要があります。クラスターに十分なスペースがあれば、手順 3 に進むことができます。
3. **コンソールを使用してノードのデプロイまたはサイズ変更を行います**。 新しいサイズのノードを収容できる大きさの Kubernetes ポッドであれば、再割り振りがスムーズに進行するはずです。ポッドを実行しているワーカー・ノードのリソースが不足している場合は、より大きな新規ワーカー・ノードをクラスターに追加してから、既存のワーカー・ノードを削除します。

| **コンポーネント** (すべてのコンテナー) | CPU  | メモリー (GB) | ストレージ (GB) |
|--------------------------------|---------------|-----------------------|------------------------|
| **ピア**                       | 1.2           | 2.4                   | 200 (ピア用に 100 GB と CouchDB 用に 100 GB を含む)|
| **CA**                         | 0.1           | 0.2                   | 20                     |
| **順序付けノード**              | 0.45          | 0.9                   | 100                    |

5 ノードの Raft 順序付けサービスをデプロイする場合は、デプロイメントの合計が 5 倍に増えることに注意してください。つまり、5 つの Raft ノードに、合計 2.25 の CPU、4.5 GB のメモリー、および 500 GB のストレージが必要になります。その結果、5 ノードの順序付けサービスは単一の 2 CPU の Kubernetes ワーカー・ノードよりも大きくなります。
{:tip}

ノードを完全にダウンさせたり除去したりすることなく料金を最低限に抑えられるように、ノードは最小の 0.001 CPU (1 milliCPU) までスケールダウンできるようになっています。 この量の CPU では、ノードは正常に機能しなくなることに注意してください。

このトピックの数値は正確であるよう努めていますが、クラスターに十分なスペースがあるように見える場合でも、ノードをデプロイできないことがあることに注意してください。 Kubernetes ダッシュボードを参照して、いつコンポーネントがデプロイされたか、またデプロイされなかった場合はエラー・メッセージを確認してください。 クラスターに十分なスペースがあるように見えるにもかかわらず、リソース不足のためにコンポーネントがデプロイされなかった場合の多くは、コンポーネントをデプロイするために追加のクラスター・リソースをデプロイする必要があります。
{:important}

## リソースの割り振り
{: #ibp-console-govern-allocate-resources}

無料クラスターのユーザーはノードに関連付けたコンテナーに**デフォルト・サイズを使用する必要があります**が、有料クラスターのユーザーはノードの作成時にサイズ値を設定できます。

コンソールの**「リソース割り振り」**パネルでは、ノードの作成に関係するさまざまなフィールドにデフォルト値が設定されています。 これらの値が選択されているのは、開始するのによい方法であるからです。 しかし、さまざまなユース・ケースがあります。 このトピックではこれらの値について検討する方法を説明しますが、最終的には、ユーザーがノードをモニターして適切なサイズを見つける必要があります。 そのため、デフォルト値以外の値が必要であることが明らかな場合を除き、最初はこれらのデフォルト値を使用し、後で調整するというのが実用的な方法です。{{site.data.keyword.blockchainfull_notm}} Platform のベースである Hyperledger Fabric のパフォーマンスおよびスケールの概要については、[Answering your questions on Hyperledger Fabric performance and scale](https://www.ibm.com/blogs/blockchain/2019/01/answering-your-questions-on-hyperledger-fabric-performance-and-scale/){: external} を参照してください。

ノードに関連するすべてのコンテナーに **CPU** と**メモリー**が割り当てられますが、ピア、順序付けプログラム、CA に関連する特定のコンテナーには**ストレージ**も割り当てられます。 ストレージについて詳しくは、[永続ストレージに関する考慮事項](/docs/services/blockchain?topic=blockchain-ibp-v2-deploy-iks#ibp-console-storage)を参照してください。

クラスター内の CPU、メモリー、およびストレージの消費量をモニターするのは、お客様が行う作業です。使用可能なリソース量より多くのリソースをブロックチェーン・ノードのリソースとして要求した場合、ノードは起動しません。ただし、既存のノードは影響を受けません。クラウド・プロバイダーとして {{site.data.keyword.cloud_notm}} を使用する場合は、コンソールおよび {{site.data.keyword.cloud_notm}} Kubernetes Service ダッシュボードで CPU とメモリーを変更できます。ただし、ノードを作成した後のストレージの変更は、{{site.data.keyword.cloud_notm}} CLI でしか実行できません。他のクラウド・プロバイダーの CPU、メモリー、およびストレージを増やす方法については、そのクラウド・プロバイダーの資料を参照してください。
{:note}

すべてのノードには、コンソールとノードの間の通信層を開始する gRPC Web プロキシー・コンテナーがあります。 このコンテナーは固定のリソース値を持ち、ノードのデプロイのために Kubernetes クラスターのスペース所要量を正確に見積もれるように「リソース割り振り」パネルに表示されます。 このコンテナーの値は変更できないので、以下のセクションでは gRPC Web プロキシーについては説明しません。

### 認証局 (CA)
{: #ibp-console-govern-CA}

トランザクション処理に積極的に関わるピアおよび順序付けプログラムとは異なり、CA は ID の登録とエンロール、および MSP の作成にのみ関わります。 つまり、CA に必要な CPU とメモリーは他より少なくなります。 CA に負荷をかけるには、ユーザーが CA に (API およびスクリプトを使用するなどの方法で) 大量の要求を送信したり、ストレージが不足するほど大量の証明書を発行したりする必要があります。 通常の操作ではこのような状況は起こりません。ただし、常にそうであるように、具体的なユース・ケースの要件を反映した値にする必要があります。

CA に関連する調整可能なコンテナーは 1 つだけです。

* **CA 自体**: ノードおよびユーザーの登録とエンロール、発行したすべての証明書のコピーの保管など、内部 CA 処理をカプセル化します。

#### 作成時の CA のサイズ設定
{: #ibp-console-govern-CA-sizing-creation}

gRPC Web プロキシー・サーバーの値は変更できないので、値を検討しなければならない CA のコンテナーは 1 つだけです。

| リソース | 増やす状況 |
|-----------------|-----------------------|
| **CA コンテナーの CPU とメモリー** | CA が大量の登録およびエンロールを行うことが予想される場合。 |
| **CA のストレージ** | この CA を使用して大量のユーザーおよびアプリケーションを登録する場合。 |

### ピア
{: #ibp-console-govern-peers}

ピアには、関連する調整可能なコンテナーが 3 つあります。

- **ピア自体**: 参加するすべてのチャネルについてのピア内部処理 (トランザクションの検証など) およびブロックチェーン (つまり、トランザクション履歴) をカプセル化します。 ピアのストレージには、ピアにインストールされたスマート・コントラクトも含まれます。
- **CouchDB**: ピアの状態データベースが格納される場所。 チャネルごとに別の状態データベースが存在することに注意してください。
- **スマート・コントラクト**: トランザクション中に、関連するスマート・コントラクトの「呼び出し」(つまり、実行) が行われます。 ピアにインストールしたすべてのスマート・コントラクトは、スマート・コントラクト・コンテナーの内部にある別のコンテナー (Docker-in-Docker コンテナー) で実行されます。  

ピアには、スマート・コントラクト・コンテナーからピア・コンテナーにログをパイプする**ログ・コレクター**用のコンテナーも含まれています。gRPC Web プロキシー・コンテナーと同様に、このコンテナーのコンピュートは調整できません。

#### 作成時のピアのサイズ設定
{: #ibp-console-govern-peers-sizing-creation}

[Kubernetes クラスターとコンソールの対話](/docs/services/blockchain/howto/ibp-console-govern.html#ibp-console-govern-iks-console-interaction)についてのセクションで触れたように、これらのピア・コンテナーにはデフォルト値を使用しておき、後でユース・ケースに応じた使用量が明らかになってから調整することをお勧めします。

| リソース | 増やす状況 |
|-----------------|-----------------------|
| **ピア・コンテナーの CPU とメモリー** | すぐに高いトランザクション・スループットを期待する場合。 |
| **ピアのストレージ** | このピアに多くのスマート・コントラクトをインストールし、ピアを多くのチャネルに参加させる場合。 このストレージには、ピアが参加するすべてのチャネルのスマート・コントラクトも格納されることに注意してください。 小規模トランザクションは 10,000 バイト (10 K) の範囲に収まると見積もっていることを念頭に置いてください。 デフォルトのストレージは 100 G であるため、拡張しなくても合計 1,000 万件のトランザクションをピア・ストレージに格納できることになります (実際は、多様なサイズのトランザクションがあるうえに、この数にはスマート・コントラクトが含まれていないので、最大数はこれよりも少なくなります)。 そのため 100 G が必要以上に大きいストレージのように思えるかもしれませんが、ストレージが比較的安価であること、また、CPU やメモリーを増やす場合と比べてストレージを増やす作業は難しいこと (コマンド・ラインが必要です) に留意してください。 |
| **CouchDB コンテナーの CPU とメモリー** | 大規模な状態データベースに対して大量の照会が行われることが予想される場合。 この状況による影響は、[索引](https://hyperledger-fabric.readthedocs.io/en/release-1.4/couchdb_as_state_database.html#couchdb-indexes){: external}を使用することで、いくらか軽減できる可能性があります。 それでもやはり、大量の照会によって CouchDB に負荷がかかり、照会とトランザクションがタイムアウトになる可能性があります。 |
| **CouchDB (台帳データ) のストレージ** | 多数のチャネルに高スループットを期待し、索引は使用しない場合。 ただし、ピアのストレージと同じく、デフォルトの CouchDB ストレージは 100 G であり、十分な大きさです。 |
| **スマート・コントラクト・コンテナーの CPU とメモリー** | チャネルに高スループットを期待する場合。特に、一度に複数のスマート・コントラクトが呼び出される場合。|

### 順序付けプログラム・ノード
{: #ibp-console-govern-ordering-nodes}

順序付けノードは状態データベースを保持することもスマート・コントラクトをホストすることもないので、ピアに比べて必要なコンテナー数が少なくなります。ただし、ブロックチェーン (トランザクション履歴) をホストします。なぜなら、ブロックチェーンがチャネル構成の保管場所であり、順序付けサービスは役割を果たすために最新のチャネル構成を認識する必要があるからです。

CA と同様に、順序付けノードには、調整可能なコンテナーが 1 つだけ関連付けられます (5 ノードの順序付けサービスをデプロイする場合は、5 セットの順序付けノード・コンテナーがあります)。

* **順序付けノード自体**: ホストするすべてのチャネルについて、順序付けプログラムの内部処理 (トランザクションの検証など) とブロックチェーンをカプセル化します。

#### 作成時の順序付けプログラムのサイズ設定
{: #ibp-console-govern-orderer-sizing-creation}

[{{site.data.keyword.cloud_notm}} Kubernetes Service とコンソールの対話](/docs/services/blockchain/howto/ibp-console-govern.html#ibp-console-govern-iks-console-interaction)のセクションで触れたように、これらの順序付けプログラム・コンテナーにはデフォルト値を使用し、使用状況が明らかになってから調整することをお勧めします。

| リソース | 増やす状況 |
|-----------------|-----------------------|
| **順序付けノード・コンテナーの CPU とメモリー** | すぐに高いトランザクション・スループットを期待する場合。 |
| **順序付けノードのストレージ** | この順序付けノードが多数のチャネルの順序付けサービスに参加することが予想される場合。 順序付けサービスは、ホストするすべてのチャネルのブロックチェーンのコピーを保持する点に留意してください。順序付けノードのデフォルト・ストレージは、ピア自体のコンテナーと同じく、100 G です。|

順序付けノードに順序付けノードとしての CPU とメモリーが十分にあることを確認することは、ベスト・プラクティスと見なされています。順序付けサービスの負荷が高すぎると、タイムアウトが発生し、トランザクションのドロップが始まり、トランザクションの再送信が必要になる可能性があります。 これにより、単一のピアの稼働が困難になる場合よりも大きな被害をネットワークが受ける可能性があります。 Raft の順序付けサービス構成では、リーダー・ノードが高負荷になると、ハートビート・メッセージの送信が停止し、リーダー選考がトリガーされ、トランザクションの順序付けが一時停止します。 同様に、フォロワー・ノードではメッセージが欠落し、リーダー選考が不要にトリガーされる可能性があります。
{:important}

## リソースの再割り振り
{: #ibp-console-govern-reallocate-resources}

ノードのサイズを変更すると、コンテナーが再作成されるので、変更が有効になるまでに待ち時間が生じる場合があります。
{:important}

前述のように、クラウド・プロバイダーが {{site.data.keyword.cloud_notm}} の場合は、[{{site.data.keyword.cloud_notm}} SysDig](https://www.ibm.com/cloud/sysdig){: external} ツールを {{site.data.keyword.cloud_notm}} Kubernetes ダッシュボードと組み合わせて、Kubernetes リソースの使用状況をモニターすることをお勧めします。ワーカー・ノードのリソースが不足していることがわかった場合は、より大きな新規ワーカー・ノードをクラスターに追加してから、既存のワーカー・ノードを削除します。
{:note}

Kubernetes クラスターに最初から十分なリソースをデプロイしておき、クラスター内のリソースを増やさなくてもリソースをデプロイおよび拡張できるようにすれば手間が減りますが、デプロイメントが大きくなるほどコストも大きくなります。選択肢について慎重に検討し、どのような選択を行うにしても、生じるトレードオフを認識する必要があります。

クラウド・プロバイダーが {{site.data.keyword.cloud_notm}} であるか、別のクラウド・プロバイダー ({{site.data.keyword.cloud_notm}} Private を使用) である場合、クラスターを手動でスケーリングして、ノードをモニターし、多くのノードを追加したり大きなノードを追加したりできます。このような処理は手間がかかることがありますが、クラウド・アカウントに課金される内容をユーザーが常に把握できるという利点があります。

{{site.data.keyword.cloud_notm}} Kubernetes Service を使用して {{site.data.keyword.cloud_notm}} にデプロイしたユーザーは、{{site.data.keyword.cloud_notm}} Kubernetes Service の**自動スケール機能**を使用することもできます。自動スケール機能を使用すると、ポッド仕様の設定とリソース要求に応じてワーカー・ノードをスケールアップ/ダウンできます。 {{site.data.keyword.cloud_notm}} Kubernetes Service 自動スケール機能とその設定方法について詳しくは、{{site.data.keyword.cloud_notm}} 資料の[クラスターのスケーリング](/docs/containers?topic=containers-ca#ca){: external}を参照してください。 自動スケール機能でリソースを調整可能にすると、使用量に応じて {{site.data.keyword.cloud_notm}} Kubernetes Service アカウントに課金されるので注意してください。

コンソールで手動でスケーリングするには、**「ノード」**ページで調整するノードをクリックし、**「使用量」**タブをクリックします。**「再割り振り (Reallocate)」**ボタンが表示されます。このボタンを押すと、ノードの作成時に表示されたタブと非常によく似た**「リソース割り振り」**タブが表示されます。 使用できるリソース量を減らす場合は、低い値を設定してそのタブの**「リソースの再割り振り (Reallocate resources)」**をクリックするだけです。そうすれば、**「サマリー」**ページに結果が表示されます。

ノードの CPU とメモリーを増やす場合は、コンソールの**「リソース割り振り」**タブを使用して値を増やします。ページ下部にある白色のボックスに、増やした新しい値を入力します。 **「リソースの再割り振り (Reallocate resources)」**をクリックすると、**「サマリー」**ページでこの値が **VPC** 量に変換されます。この値を使用して請求額が算出されます。次に、Kubernetes クラスターに移動して、この再割り振りのために十分なリソースがクラスターにあることを確認する必要があります。十分ある場合は、**「リソースの再割り振り (Reallocate resources)」**をクリックできます。 十分なリソースがない場合は、クラスターのサイズを増やす必要があります。

ストレージを増やすために使用する方法は、クラスターに選択したストレージ・クラスによって異なります。 詳しくは、クラウド・プロバイダーの資料を参照してください。{{site.data.keyword.cloud_notm}} では、このトピックを[ストレージ・オプション](/docs/containers?topic=containers-kube_concepts#kube_concepts){: external}と呼んでいます。{{site.data.keyword.cloud_notm}} では、ピアまたは順序付けプログラムのストレージが満杯に近付いたら、より大きなファイル・システムを持つ新しいピアまたは順序付けプログラムをデプロイし、同じチャネル上の他のコンポーネントと同期する必要があることに注意してください。

{{site.data.keyword.cloud_notm}} では、CPU およびメモリーはコンソールを使用して増やせます ({{site.data.keyword.cloud_notm}} Kubernetes Service クラスターに使用可能なリソースがある場合)。ただし、ストレージは {{site.data.keyword.cloud_notm}} CLI を使用して増やす必要があります。その方法を示すチュートリアルについては、[既存のストレージ・デバイスのサイズと IOPS の変更](/docs/containers?topic=containers-file_storage#file_change_storage_configuration){: external}を参照してください。 {{site.data.keyword.cloud_notm}} 以外のクラウド・プロバイダーを使用している場合は、そのプロバイダーの資料で CPU、メモリー、およびストレージを増やすプロセスを確認してください。

## 組織 MSP 定義の更新
{: #ibp-console-govern-update-msp}

時間が経つと、組織 MSP 定義の更新が必要になることがあります。例えば、組織管理者を追加したり、コンソールでの MSP の表示名を変更したりする必要が生じます。

- 単に、**「組織」**タブから既存の MSP 定義を単にエクスポートし、生成された JSON ファイルを編集して、表示名や既存の証明書を変更したり新しい証明書を追加したりするだけです。`msp_id` フィールドの変更はお勧めしません。変更すると、ネットワークに破壊的な変更が生じる可能性があります。
- **「組織」**タブで MSP 定義をクリックして開き、**「ファイルの追加」**をクリックして、新しい JSON ファイルをアップロードします。
- **「MSP 定義の更新 (Update MSP definition)」**をクリックします。変更はただちに有効になります。

## チャネル構成の更新
{: #ibp-console-govern-update-channel}

チャネルの作成とチャネルの更新には、ユーザーが、チャネルの構成を自分のユース・ケースにも可能な限り適切な構成にできるようにする、という同じ目的がありますが、実際にはこの 2 つのプロセスは、コンソール内の**タスクとして**は大きく異なります。 [チャネルの作成](/docs/services/blockchain/howto/ibp-console-build-network.html#ibp-console-build-network-create-channel)についての資料で述べたように、これは**単一の組織**によって実行されるプロセスです。 組織が、チャネルの順序付けサービスとなる順序付けサービスのコンソーシアムのメンバーである限り、それらの組織は任意の方法でチャネルを作成できます。 この組織は、チャネルに任意の名前を付け、任意の組織を追加し (組織がコンソーシアムのメンバーである場合)、それらの組織に権限を割り当て、アクセス制御リストを設定するなどの操作も実行できます。

その他の組織は、このチャネルに参加するかどうか (例えば、ピアをチャネルに参加させるかどうか) を選択できますが、チャネル・パラメーターを選択するコラボレーション・プロセスは、組織がコンソールを使用してチャネルを作成する前に、アウト・オブ・バンドで行われると想定されています。

チャネルの更新は異なります。 これは**コンソール内**で行われ、{{site.data.keyword.blockchainfull_notm}} Platform の動作の基礎となる、コラボレーション・ガバナンス手順に従います。 このコラボレーション・プロセスでは、チャネル内の管理ロールを持つ組織に対し、チャネル構成の更新要求が送信されます。 これらの組織は、チャネル・**オペレーター**とも呼ばれます。

チャネルを更新するには、**「チャネル」**タブ内のチャネルをクリックします。 ページ上部のチャネル名の横にある**「設定」**ボタンをクリックします。 チャネルの作成に使用したパネルと非常によく似た外観のパネルが表示されます。

### 更新可能なチャネル構成パラメーター
{: #ibp-console-govern-update-channel-available-parameters}

チャネルの作成後は、チャネルの構成パラメーターの一部を変更することはできますが、すべてを変更することはできません。 また、更新できるパラメーターは 1 つのみであり、チャネルの作成中には使用できません。

**チャネル名**がグレー表示され、編集できないことからこれが分かります。 これは、チャネルの作成後は、チャネル名を変更できないという事実を反映しています。 また、順序付けサービスの表示名も表示されないことを確認してください。これは、チャネルの作成後はチャネルの順序付けサービスも変更できないためです。

ただし、次のチャネル構成パラメーターは変更できます。

* **「組織」**。 パネルのこのセクションは、チャネルでの組織の追加方法または削除方法を示しています。 追加できる組織が、ドロップダウン・リストに表示されます。 組織をチャネルに追加するには、その組織が、順序付けサービスのコンソーシアムのメンバーになっている必要があることに注意してください。 組織をコンソーシアムに追加する方法について詳しくは、[取引できる組織のリストに組織を追加する](/docs/services/blockchain/howto/ibp-console-build-network.html#ibp-console-build-network-add-org)を参照してください。

* **「チャネル更新ポリシー (Channel update policy)」**。 チャネルの更新ポリシーは、(チャネル内の組織の合計数のうち) チャネル構成の更新を承認する必要がある組織の数を指定します。 共同管理とチャネル構成の更新の効率的な処理間の良好なバランスが保たれるようにするために、このポリシーを管理者の大多数に設定することを検討してください。 例えば、チャネルに 5 人の管理者がいる場合は、`3 out of 5` を選択します。

* **「ブロック・カット・パラメーター (Block cutting parameters)」**。 (拡張オプション) デフォルトのブロック・カット・パラメーターに対する変更には、順序付けサービス組織の管理者による署名が必要なため、これらのフィールドはチャネル作成パネルには表示されません。 ただし、このチャネル構成はチャネル内の関連組織すべてに送信されるため、ブロック・カット・パラメーターへの変更を含むチャネル構成の更新要求を送信できます。 これらのフィールドは、順序付けサービスによって新規ブロックがカットされる条件を決定します。 これらのフィールドがブロックのカット条件にどのように影響するかについて詳しくは、[ブロック・カット・パラメーター](/docs/services/blockchain/howto/ibp-console-govern.html#ibp-console-govern-orderer-tuning-batch-size)を参照してください。

* **「アクセス制御リスト」**。 (拡張オプション) リソースに対してきめ細かく制御を指定する場合、リソースへのアクセス権限を組織および組織内の役割に制限できます。 例えば、リソース `ChaincodeExists` へのアクセス権限を `Application/Admins` に設定すると、アプリケーションの管理者のみが `ChaincodeExists` リソースにアクセスできます。

リソースへのアクセス権限を特定の組織に制限する場合、その組織のみがリソースへアクセスできることに注意してください。 別の組織がそのリソースへアクセスできるようにする場合は、以下のフィールドを使用してそれらを 1 つずつ追加する必要があります。 そのため、アクセス制御を決めるときは慎重に検討してください。 特定の方法で特定のリソースに対するアクセス権限を制限すると、チャネル機能に非常に悪影響を及ぼす可能性があります。
{:important}

コンソールでは、複数組織を所有して制御できるのは単一ユーザーのみであるため、**「チャネル更新者組織 (Channel updater organization)」**セクションでチャネル更新に署名する際には、使用している組織を指定する必要があります。 このチャネルで複数の組織を所有している場合は、署名する組織として、チャネルで所有している任意の組織を選択できます。 選択した**チャネル更新ポリシー**によっては、所有しているその他 1 つ以上の組織として、要求に署名するよう求める通知が表示されることがあります。

このチャネルの順序付けサービス組織を所有しており、いずれかの**ブロック・カット・パラメーター**を変更しようとした場合、その順序付けサービス組織のフィールドが表示されます。 ドロップダウン・リストから、関連する順序付けサービス組織の MSP を選択してください。 順序付けサービス組織の管理者でない場合でも、いずれかのブロック・カット・パラメーターを変更する要求を行うことはできますが、その要求は順序付けサービスの管理者に送信され、管理者によって署名される必要があります。

### 署名収集フロー
{: #ibp-console-govern-update-channel-signature-collection}

署名を検証するには、チャネル上の組織が、自分の組織を表す MSP を (JSON フォーマットで) チャネル上の他の組織にエクスポートし、また、他の組織の MSP をインポートする必要があります。 MSP をエクスポートするには、(**「組織」**画面の) MSP でダウンロード・ボタンをクリックし、アウト・オブ・バンドで MSP を他の組織に送信します。 MSP の JSON を受信したら、**「組織」**画面を使用してそれをインポートします。
{: important}

チャネル構成の更新要求が行われると、更新要求に署名する権限を持つチャネル内の組織に要求が送信されます。 例えば、チャネルに 5 人のオペレーター (チャネル管理者) がいる場合は、5 人全員に送信されます。 チャネル構成の更新が承認されるためには、**チャネル更新ポリシー**に示されているチャネル・オペレーターの数が満たされる必要があります。 このポリシーで `3 out of 5` と規定されている場合、チャネル構成の更新は 5 人のオペレーター全員に送信され、そのうち 3 人が署名すれば、新しいチャネル構成の更新が有効になります。

署名が必要な更新があることを確認し、署名するこのプロセスは、コンソールの右上にある**「通知」**ボタン (ベルのようなボタン) を使用して処理されます。 **「通知」**ボタンに青いドットが表示されている場合、評価が必要な未処理要求があるか、チャネル更新イベントの通知があります。

**「通知」**ボタンをクリックすると、実行できる 1 つ以上のアクションが示されます。

* **要注意 (Needs attention)**: 現行ユーザーが (ピア組織または順序付けサービス組織として) 要求に署名するか、要求を送信する必要があります (必要なすべての署名が既に収集されている場合)。
* **オープン**: **要注意 (Needs attention)** の要求すべてと、現行ユーザーは署名済みであるが、まだ 1 人以上の他のチャネル・メンバーの署名を必要とする要求が含まれます。
* **クローズ**: 送信された要求。これらの項目に対して実行すべきアクションはありません。これらは表示のみが可能です。
* **すべて**: オープン要求とクローズ要求の両方が含まれます。

チャネル構成の更新要求が行われた場合は、`「チャネル構成のレビューおよび更新 (Review and update channel configuration)」`をクリックして、提案されているチャネル構成の更新の変更、または行われた変更 (新しいチャネル構成が承認済みの場合) を確認できます。 チャネルのオペレーターは、チャネル構成の更新要求を承認するために十分な数の署名がまだ収集されていない場合、更新要求に署名できます。

チャネル構成の更新への署名は必須ではありませんが、チャネル更新を**却下する**ために署名することはできません。 チャネル構成の更新を承認しない場合は、単にパネルを閉じ、アウト・オブ・バンドで他のチャネル・オペレーターに連絡して、懸念事項を伝えることができます。 ただし、チャネル更新ポリシーを満たすために十分な数のチャネルのオペレーターが更新を承認した場合は、新しい構成が有効になります。
{:note}

### アンカー・ピアの構成
{: #ibp-console-govern-channels-anchor-peers}

サービス・ディスカバリーとプライベート・データを使用するには組織間[ゴシップ](https://hyperledger-fabric.readthedocs.io/en/release-1.4/gossip.html){: external}を有効にする必要があるため、組織ごとにアンカー・ピアが存在しなければなりません。 このアンカー・ピアは特殊な**タイプ**のピアではありません。組織が他の組織に周知するピアに過ぎず、組織間ゴシップを開始します。 そのため、少なくとも 1 つの[アンカー・ピア](https://hyperledger-fabric.readthedocs.io/en/release-1.4/gossip.html#anchor-peers){: external}を、コレクション定義に含まれている組織ごとに定義する必要があります。

ピアをアンカー・ピアとして構成するには、**「チャネル」**タブをクリックして、スマート・コントラクトがインスタンス化されたチャネルを開きます。
 - **「チャネルの詳細 (Channel details)」**タブをクリックします。
 - アンカー・ピアの表までスクロールダウンし、**「アンカー・ピアの追加 (Add anchor peer)」**をクリックします。
 - 組織のアンカー・ピアにするピアを、コレクション定義に含まれている各組織から 1 つ以上選択します。 冗長性を確保するために、コレクションに含まれている各組織から複数のピアを選択することを検討すると良いでしょう。

## 順序付けプログラムの調整
{: #ibp-console-govern-orderer-tuning}

ブロックチェーン・プラットフォームのパフォーマンスは、トランザクション・サイズ、ブロック・サイズ、ネットワーク・サイズ、ハードウェアの制限など、多くの変動要素の影響を受けます。 順序付けプログラムのノードには、順序付けプログラムのスループットとパフォーマンスを制御するために使用できる一連のチューニング・パラメーターが含まれています。  これらのパラメーターを使用して、高い頻度で多数の小さいトランザクションを処理するのか、低い頻度で到着する少数の大きいトランザクションを処理するのかに応じて、順序付けプログラムによるトランザクションの処理方法をカスタマイズできます。 基本的には、トランザクションのサイズ、数量、および到着レートに応じてブロックをカットするタイミングを制御できます。

コンソールの**「ノード」**タブで順序付けプログラムのノードをクリックしてから**「設定」**アイコンをクリックすると、以下のパラメーターが表示されます。 **「詳細」**ボタンをクリックすると、順序付けプログラムの**「詳細なチャネル構成 (Advanced channel configuration)」**が開きます。

### ブロック・カット・パラメーター
{: #ibp-console-govern-orderer-tuning-batch-size}

以下の 3 つのパラメーターが組み合わさって、ブロックをカットするタイミングを制御します。1 つのブロックの最大トランザクション数とブロック・サイズ自体の設定の組み合わせが制御の基礎になります。

- **絶対最大バイト数 (Absolute max bytes)**  
  この値には、順序付けプログラムでカットできる最大ブロック・サイズ (バイト単位) を設定します。  `絶対最大バイト数`の値より大きいトランザクションは許可されません。 通常、この設定値は`推奨最大バイト数`の 2 倍から 10 倍にすることをお勧めします。    
  **注**: 最大サイズは 99 MB です。
- **最大メッセージ数 (Max message count)**   
  この値には、1 つのブロックに含めることができるトランザクションの最大数を設定します。
- **推奨最大バイト数 (Preferred max bytes)**  
  この値には、理想的なブロック・サイズ (バイト単位) を設定します。`絶対最大バイト数`より小さくする必要があります。 エンドースメントを含まない最小のトランザクション・サイズは約 1 KB です。  必要なエンドースメントごとに 1 KB を加えると、通常のトランザクション・サイズは約 3 KB から 4 KB になります。 そのため、`推奨最大バイト数`の値には、`最大メッセージ数 * 予想される平均トランザクション・サイズ`に近い値を設定することをお勧めします。 ランタイム時に、このサイズを超えるブロックが (可能な限り) 作成されなくなります。 このサイズを超えるブロックになるトランザクションが到着すると、ブロックはカットされ、そのトランザクション用に新規ブロックが作成されます。 ただし、この値を超えても`絶対最大バイト数`を超えないトランザクションが到着した場合は、そのトランザクションは含められます。 `推奨最大バイト数`を超えるブロックが到着した場合、そのブロックにはトランザクションは 1 つしか含められません。また、そのトランザクション・サイズが`絶対最大バイト数`を超えることはありません。

これらのパラメーターを組み合わせて構成することで、順序付けプログラムのスループットを最適化できます。

### バッチ・タイムアウト
{: #ibp-console-govern-orderer-tuning-batch-timeout}

**タイムアウト**値に、最初のトランザクションが到着してからブロックをカットするまで待つ時間を秒単位で設定します。 この値の設定が小さすぎると、バッチが推奨サイズに達しないリスクがあります。 この値の設定が大きすぎると、順序付けプログラムがブロックを待機し、全体的なパフォーマンスが低下する可能性があります。 一般には、`バッチ・タイムアウト`の値には、`最大メッセージ数 / 1 秒あたりの最大トランザクション数`以上の値を設定することをお勧めします。

これらのパラメーターを変更しても、順序付けプログラムの既存のチャネルの動作には影響しません。順序付けプログラムの構成に加えた変更は、その順序付けプログラムで作成する新規チャネルにのみ適用されます。
{:important}
